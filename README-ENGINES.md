# WhisperAPI - Engine Comparison Guide

WhisperAPI supports three different Whisper engine implementations, each with unique characteristics and optimal use cases.

## ğŸš€ Available Engines

### 1. whisper.cpp (Default)
**Original C++ implementation**
- âš¡ **Fastest startup time**
- ğŸ’¾ **Low memory usage**
- ğŸ”§ **Easy installation** (no Python dependencies)
- ğŸ“± **CPU optimized**
- ğŸ¯ **Best for**: Production environments with limited resources

**Configuration:**
```bash
WHISPER_ENGINE=whisper.cpp
```

### 2. faster-whisper
**Python implementation with CTranslate2**
- âš¡ **Faster inference** than whisper.cpp
- ğŸ¯ **Better accuracy** for complex audio
- ğŸ **Requires Python** and additional dependencies
- ğŸ’¾ **Medium memory usage**
- ğŸ¯ **Best for**: Balanced performance and accuracy

**Configuration:**
```bash
WHISPER_ENGINE=faster-whisper
FASTER_WHISPER_DEVICE=cpu  # or cuda
FASTER_WHISPER_COMPUTE_TYPE=int8
```

### 3. insanely-fast-whisper
**Transformers-based implementation**
- ğŸ† **Fastest processing** (especially with GPU)
- ğŸ¤– **Latest transformer optimizations**
- ğŸ–¥ï¸ **GPU accelerated** (CUDA recommended)
- ğŸ“¦ **Batch processing support**
- ğŸ¯ **Best for**: High-throughput scenarios with GPU

**Configuration:**
```bash
WHISPER_ENGINE=insanely-fast-whisper
INSANELY_FAST_WHISPER_MODEL=openai/whisper-large-v3-turbo
INSANELY_FAST_WHISPER_DEVICE=auto  # auto, cpu, cuda
INSANELY_FAST_WHISPER_TORCH_DTYPE=auto  # auto, float16, float32
INSANELY_FAST_WHISPER_BATCH_SIZE=24
INSANELY_FAST_WHISPER_CHUNK_LENGTH_S=30
```

## ğŸ“Š Performance Comparison

| Feature | whisper.cpp | faster-whisper | insanely-fast-whisper |
|---------|------------|----------------|----------------------|
| **Startup Time** | ğŸ¥‡ Fastest | ğŸ¥ˆ Medium | ğŸ¥‰ Slowest |
| **Processing Speed (CPU)** | ğŸ¥‰ Baseline | ğŸ¥ˆ 2-4x faster | ğŸ¥‡ 2-8x faster |
| **Processing Speed (GPU)** | âŒ N/A | ğŸ¥ˆ Fast | ğŸ¥‡ Very Fast |
| **Memory Usage** | ğŸ¥‡ Low | ğŸ¥ˆ Medium | ğŸ¥‰ High |
| **Installation** | ğŸ¥‡ Simple | ğŸ¥ˆ Medium | ğŸ¥‰ Complex |
| **Dependencies** | ğŸ¥‡ Minimal | ğŸ¥ˆ Python + libs | ğŸ¥‰ Python + PyTorch |
| **Accuracy** | ğŸ¥ˆ Good | ğŸ¥‡ Very Good | ğŸ¥‡ Very Good |
| **Word Timestamps** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Batch Processing** | âŒ No | âš ï¸ Limited | âœ… Yes |

## ğŸ› ï¸ Installation

### Prerequisites (All Engines)
```bash
# Node.js dependencies
npm install

# FFmpeg (required for all engines)
sudo apt update && sudo apt install ffmpeg
```

### whisper.cpp (Default)
```bash
# Already installed with npm install
# No additional setup required
```

### faster-whisper
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install faster-whisper
```

### insanely-fast-whisper
```bash
# Use the automated installation script
./scripts/install-insanely-fast-whisper.sh

# Or manual installation:
source venv/bin/activate
pip install torch transformers librosa numpy
```

## ğŸ§ª Testing & Benchmarking

### Quick Engine Test
Test all three engines with the same audio file:
```bash
node test-engine-toggle.js
```

### Comprehensive Benchmark
Run detailed performance comparison:
```bash
node test-engine-benchmark.js
```

The benchmark will test each engine and generate a detailed report including:
- Processing times
- Memory usage
- Accuracy comparison
- Performance rankings

## ğŸ¯ Choosing the Right Engine

### Use **whisper.cpp** when:
- ğŸƒâ€â™‚ï¸ You need fast startup times
- ğŸ’° Running on limited hardware/budget
- ğŸ³ Deploying in containers
- ğŸ”§ You want minimal dependencies
- ğŸ“± CPU-only environment

### Use **faster-whisper** when:
- âš–ï¸ You need balanced performance/accuracy
- ğŸ¯ Processing quality is important
- ğŸ’¾ You have moderate memory available
- ğŸ Python environment is acceptable

### Use **insanely-fast-whisper** when:
- ğŸ† Maximum speed is priority
- ğŸ–¥ï¸ GPU acceleration is available
- ğŸ“ˆ High-throughput processing needed
- ğŸ¤– You want latest AI optimizations
- ğŸ“¦ Batch processing is required

## ğŸ”§ Configuration Examples

### Development Setup (Quick Start)
```bash
# .env
WHISPER_ENGINE=whisper.cpp
AUTO_DOWNLOAD_MODEL=small
MAX_WORKERS=2
```

### Production Setup (Balanced)
```bash
# .env
WHISPER_ENGINE=faster-whisper
FASTER_WHISPER_DEVICE=cpu
FASTER_WHISPER_COMPUTE_TYPE=int8
AUTO_DOWNLOAD_MODEL=large-v3-turbo
MAX_WORKERS=4
```

### High-Performance Setup (GPU)
```bash
# .env
WHISPER_ENGINE=insanely-fast-whisper
INSANELY_FAST_WHISPER_MODEL=openai/whisper-large-v3-turbo
INSANELY_FAST_WHISPER_DEVICE=auto
INSANELY_FAST_WHISPER_TORCH_DTYPE=float16
INSANELY_FAST_WHISPER_BATCH_SIZE=24
MAX_WORKERS=2  # Fewer workers needed with GPU
```

## ğŸ“ˆ Performance Tips

### General Optimization
- Use appropriate model size for your accuracy needs
- Monitor memory usage with `GET /system-report`
- Enable auto-scaling: `AUTO_SCALE=true`

### whisper.cpp Optimization
```bash
# Use smaller models for speed
AUTO_DOWNLOAD_MODEL=small  # or tiny for maximum speed

# Increase workers for CPU parallelization
MAX_WORKERS=4
```

### faster-whisper Optimization
```bash
# Use int8 for speed, float16 for accuracy
FASTER_WHISPER_COMPUTE_TYPE=int8

# Enable GPU if available
FASTER_WHISPER_DEVICE=cuda
```

### insanely-fast-whisper Optimization
```bash
# Increase batch size for GPU utilization
INSANELY_FAST_WHISPER_BATCH_SIZE=32

# Use float16 for GPU speed
INSANELY_FAST_WHISPER_TORCH_DTYPE=float16

# Adjust chunk length for memory/speed balance
INSANELY_FAST_WHISPER_CHUNK_LENGTH_S=30
```

## ğŸ› Troubleshooting

### Common Issues

**whisper.cpp startup fails:**
```bash
# Check model files
ls models/
# Re-download if needed
rm models/* && npm run postinstall
```

**faster-whisper import errors:**
```bash
# Ensure virtual environment is activated
source venv/bin/activate
pip install --upgrade faster-whisper
```

**insanely-fast-whisper CUDA issues:**
```bash
# Check CUDA installation
nvidia-smi
python3 -c "import torch; print(torch.cuda.is_available())"

# Reinstall PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Getting Help

1. Check server logs: `tail -f server.log`
2. Run health check: `curl http://localhost:3001/health`
3. View system report: `curl http://localhost:3001/system-report`
4. Test individual engines: `node test-engine-toggle.js`

## ğŸš€ Quick Start

1. **Choose your engine** based on requirements above
2. **Install dependencies** using appropriate method
3. **Configure .env** with your engine settings
4. **Start server**: `node src/server.js`
5. **Test**: `node test-engine-toggle.js`
6. **Benchmark**: `node test-engine-benchmark.js`

Happy transcribing! ğŸ¤âœ¨