AUTO_DOWNLOAD_MODEL=small
PORT=3001
MAX_WORKERS=1
MAX_FILE_SIZE=100MB
# WHISPER_MODEL_PATH=./node_modules/nodejs-whisper/cpp/whisper.cpp/models/ggml-large-v3-turbo.bin
UPLOAD_DIR=./uploads
TEMP_DIR=./temp
WHISPER_LANGUAGE=pt

# Whisper Engine Configuration
# WHISPER_ENGINE: Choose between 'whisper.cpp', 'faster-whisper', and 'insanely-fast-whisper'
# - whisper.cpp: Original C++ implementation (default, faster startup)
# - faster-whisper: Python implementation with CTranslate2 (faster inference, requires Python)
# - insanely-fast-whisper: Transformers-based implementation (fastest, requires GPU for best performance)
WHISPER_ENGINE=whisper.cpp

# Faster-Whisper specific settings (only used when WHISPER_ENGINE=faster-whisper)
FASTER_WHISPER_DEVICE=cpu
FASTER_WHISPER_COMPUTE_TYPE=int8

# Insanely-Fast-Whisper specific settings (only used when WHISPER_ENGINE=insanely-fast-whisper)
INSANELY_FAST_WHISPER_MODEL=openai/small
INSANELY_FAST_WHISPER_DEVICE=auto
INSANELY_FAST_WHISPER_TORCH_DTYPE=auto
INSANELY_FAST_WHISPER_BATCH_SIZE=24
INSANELY_FAST_WHISPER_CHUNK_LENGTH_S=30

# Worker Auto Scaler Configuration
AUTO_SCALE=true
AUTO_SCALE_INTERVAL=60000
AUTO_SCALE_CACHE_TTL=30000
AUTO_SCALE_MIN_WORKERS=1
AUTO_SCALE_MAX_WORKERS=8
AUTO_SCALE_MEMORY_THRESHOLD=100
AUTO_SCALE_CPU_THRESHOLD=100

# Whisper Model Configuration
# AUTO_DOWNLOAD_MODEL: Model to auto-download if not found (overrides WHISPER_MODEL_PATH)
# Available models for whisper.cpp: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large, large-v1, large-v2, large-v3, large-v3-turbo
# Available models for faster-whisper: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, large-v3, large-v3-turbo, distil-medium.en, distil-large-v2, distil-large-v3
# WHISPER_MODEL_PATH: Manual path to model file (optional, auto-generated from AUTO_DOWNLOAD_MODEL if not set, only for whisper.cpp)
# Default auto-download: large-v3-turbo (good balance of speed and accuracy)

# Auto Scaler Configuration Details:
# AUTO_SCALE: Enable/disable automatic worker scaling (true/false)
# AUTO_SCALE_INTERVAL: Interval in milliseconds to check for scaling needs (default: 60000ms = 1 minute)
# AUTO_SCALE_CACHE_TTL: Cache TTL for system resource detection (default: 30000ms = 30 seconds)
# AUTO_SCALE_MIN_WORKERS: Minimum number of workers to maintain (default: 1)
# AUTO_SCALE_MAX_WORKERS: Maximum number of workers allowed (default: 8)
# AUTO_SCALE_MEMORY_THRESHOLD: Memory usage percentage threshold for scaling down (default: 80%)
# AUTO_SCALE_CPU_THRESHOLD: CPU load percentage threshold for scaling down (default: 80%)
