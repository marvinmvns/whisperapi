AUTO_DOWNLOAD_MODEL=small
PORT=3001
MAX_WORKERS=1
MAX_FILE_SIZE=100MB
# WHISPER_MODEL_PATH=./node_modules/nodejs-whisper/cpp/whisper.cpp/models/ggml-large-v3-turbo.bin
UPLOAD_DIR=./uploads
TEMP_DIR=./temp
WHISPER_LANGUAGE=pt

# Worker Auto Scaler Configuration
AUTO_SCALE=true
AUTO_SCALE_INTERVAL=60000
AUTO_SCALE_CACHE_TTL=30000
AUTO_SCALE_MIN_WORKERS=1
AUTO_SCALE_MAX_WORKERS=8
AUTO_SCALE_MEMORY_THRESHOLD=100
AUTO_SCALE_CPU_THRESHOLD=100

# Whisper Model Configuration
# AUTO_DOWNLOAD_MODEL: Model to auto-download if not found (overrides WHISPER_MODEL_PATH)
# Available models: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large, large-v1, large-v2, large-v3, large-v3-turbo
# WHISPER_MODEL_PATH: Manual path to model file (optional, auto-generated from AUTO_DOWNLOAD_MODEL if not set)
# Default auto-download: large-v3-turbo (good balance of speed and accuracy)

# Auto Scaler Configuration Details:
# AUTO_SCALE: Enable/disable automatic worker scaling (true/false)
# AUTO_SCALE_INTERVAL: Interval in milliseconds to check for scaling needs (default: 60000ms = 1 minute)
# AUTO_SCALE_CACHE_TTL: Cache TTL for system resource detection (default: 30000ms = 30 seconds)
# AUTO_SCALE_MIN_WORKERS: Minimum number of workers to maintain (default: 1)
# AUTO_SCALE_MAX_WORKERS: Maximum number of workers allowed (default: 8)
# AUTO_SCALE_MEMORY_THRESHOLD: Memory usage percentage threshold for scaling down (default: 80%)
# AUTO_SCALE_CPU_THRESHOLD: CPU load percentage threshold for scaling down (default: 80%)
